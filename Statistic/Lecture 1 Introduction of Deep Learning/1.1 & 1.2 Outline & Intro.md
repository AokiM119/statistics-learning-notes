## 1. 1 Class Outline

**Lecture 7 - Self-supervised learning**
Pretrain: develop general purpose learning knowledge
先学习基本功：图片左右翻转、变色，通过没有标注的资料学习基本任务
可以在下游任务上得到好的结果
Pretrain Model (Foundation Model) VS Downstream Tasks
Foundation Model里很有名的一个模型：BERT
预训练模型与下游任务的关系，就类似于操作系统和上层应用（Application）的关系。

**Lecture 6: Generative Adversarial Network**
找函数，需要大量成对的数据样本。有Adversarial技术，只需要大量的X和Y，不需要成对的关系，机器就能把他们之间的关联找出来。

**Lecture 12: Reinforcement Learning**
出现在你不知道如何标注资料的时候（人类也无法给出最好最确切的标注）

**Lecture 8: Anomaly Detection**
异常检测，让机器具备回答“我不知道”的能力

**Lecture 9: Explainable AI**
为什么AI觉得这是个正确的答案？要给出解释原因。
There is a chance Machine discriminate Pokemon and Digimon based on the background color…

**Lecture 10: Model Attack**
图片注入噪声之后，辨识结果完全不同

**Lecture 11: Domain Adaptation**
Training Data & Testing Data: 相差不大还OK，如果测试资料变成彩色，辨识准确度就会暴跌

**Lecture 13: Network Compression**
模型的压缩，跑在mobile device上面

**Lecture 14: Life-long Learning**
为什么全能的（一直学下去的）AI没有出现

**Lecture 15: Meta Learning**
机器不再使用人发明的演算法，从过去的学习经验里发明新的演算法。
Few shot learning - 很很少的标注资料就行机器学习
Few shot learning is usually achieved by meta learning.

## 1.2 什么是机器学习？

**Machine Learning ~= Looking for Function**
机器学习的本质是，利用机器帮助我们找到一个函数（模型）。这个模型能够帮助我们进行预测或分析。

***Different types of Functions***
- Regression: The function outputs a scalar
- Classification: Given options (classes), the function outputs the correct one.
- Structured Learning
	- Create something with structure (image, document)

机器如何找“那个函数”？
***3 steps to find that function:***
1. ***Function with Unknown Parameter | 我们要写出一个带有未知参数的函数（即要猜测函数方程的结构）***
	1. Model (Function): This function should be chosen based on domain knowledge
	2. Feature: input
	3. Weight and Bias: unknown parameters (learned from data)
2. ***Define Loss from Training Data | 定义损失函数***
	1. Loss is a function of [**model's parameters**]: L(b,w)
	2. Loss value shows how good a set of parameter value is.
	3. 损失函数有不同的表现形式。有Mean Absolute Error (MAE)、Mean Square Error (MSE)等。
3. ***Optimization: find the model's parameters that will make the Loss value minimum.***
	1. How to find the right parameters
	2. 梯度下降法，其实只是求解Loss函数最值的一种算法。梯度下降法并无法确保能够找到真正的最优解。有可能最后会陷入Local Minima而无法找到Global Minima.
	3. Loss函数有两个参数的情景：可能就是微分公式需要跟着调整（二元微分）
	4. Hyperparameter: 梯度下降的步长，是需要自己设定的参数，这类的参数归属于Hyoerparameter
	5. When to stop?
	6. 可以自定义阈值，或者步进值

总结：
1. Function with unknowns
2. Define Loss from training data
3. Optimization

Linear models are too simple… we need more sophisticated modes.
Linear models have severe limitations: Model Bias
We need functions that have more flexibility and parameters…
**Piecewise Linear Curves** = constant + sum of (sigmoid?)
More piece requires more sigmoids…
New model allow us to have more features

J = No. of features
I = No. of sigmoid function
把线性的组合，换成了sigmoid函数的组合

Linear algebra expression of Function
R = b +Wx
A = sigmoid®
当做好线性代数的解包后，把所有参数压缩到一个整体向量（Theta）中：

把样本随机分成几个batch
- Epoch的概念

激活函数
- Sigmoid
- ReLu
还能够在输入层后面，加上其他层。同样也是用Sigmoid/ReLu函数。——加不同层的本质，好像就是重新建构模型函数，但是不知道这样做的道理为何。

为什么不把Network变宽，而要把Network变深呢？

**线性回归模型**

线性回归是最简单的预测模型，它假设输入特征x和输出y之间存在线性关系。这种模型的表达能力有限，因为它只能捕捉数据中的线性依赖。形式通常为：y=wx+b

其中w是权重，b是偏置。

**引入非线性**

现实世界的数据往往是非线性的，单纯的线性模型无法有效捕捉这种复杂的数据结构。为了解决这个问题，我们可以引入非线性函数，即激活函数，比如Sigmoid或ReLU，来增加模型的表达能力。激活函数允许模型捕捉输入和输出之间的非线性关系。

**多层神经网络**

当我们将神经网络从单层扩展到多层（即深度学习）时，模型的表达能力显著增加。每一层可以被视为在进行一种更复杂的特征转换，每个神经元都可以从前一层学到的特征中进一步提取信息。多层结构允许逐层构建从简单到复杂的特征表示。

激活函数的作用

非线性引入：如果没有激活函数，不论神经网络有多少层，输出仍然是输入的线性组合，这限制了网络的表达能力。通过引入非线性激活函数，每一层都可以学到非线性的数据表示，极大地增强了网络的学习能力。

增加决策边界的复杂性：在分类问题中，非线性激活函数可以帮助模型学习更复杂的决策边界。

**为什么这种方法有效？**

多层神经网络的有效性可以从多个角度来解释：

- 统计学和机器学习理论：理论上已经证明，神经网络特别是深度神经网络可以近似任何复杂的连续函数（通用近似定理）。
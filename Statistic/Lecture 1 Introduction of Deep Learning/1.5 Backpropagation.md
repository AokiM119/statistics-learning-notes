反向传播（Back Propagation，简称BP）是深度学习中用于训练神经网络的核心算法。它通过计算损失函数（Loss Function）对模型参数的梯度，利用梯度下降法（Gradient Descend）优化参数，使模型输出尽可能接近真实值。


以下是其核心原理和步骤的清晰解释：

### **1. 核心思想**

反向传播的核心是**链式法则（Chain Rule）**，即通过从输出层向输入层逐层反向传播误差，计算每一层参数的梯度，从而高效更新权重。

- **为什么需要反向传播？**  
    神经网络通常包含大量参数，手动计算每个参数的梯度极其低效。反向传播通过自动微分，利用计算图的拓扑结构高效计算所有参数的梯度。
    这里可以对比参考线性回归方程。对于简单的线性回归方程来说，可以用解析的形式计算出参数的损失函数；对于复杂结构的神经网络来说，使用反向传播可以规避掉这个麻烦的步骤。


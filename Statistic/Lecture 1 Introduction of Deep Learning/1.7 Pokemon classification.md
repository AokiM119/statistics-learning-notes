Classification as Regression?

Binary classification as example:

Class 1 means the target = 1; class 2 means the target = -1.

***下方为图片

  

不能直接用regression的思维做分类：

右图右下角的那些点，会让紫色的线变成ideal model (function)，因为它的主旨就是减少loss function

对于multiple class来说，随便assign值然后使用回归方法也是不行的。

Example:

Class 1 = 1 ; Class 2 = 2 ; Class 3 = 3 ….

其实暗中里默认了类别2和类别3有更近的关系。

想象一个场景……

有AB两个盒子，里面分别随机装蓝球和绿球。

此时有了一个蓝球，试问是它大概率数据AB哪个盒子。

从概率角度解决这个问题，我们需要以下的Prior概率：

P(A), P(B)

P(Blue|A) - A类中有蓝色feature的概率

P(Green|A)- A类中有绿色feature的概率

P(Blue|B)- B类中有蓝色feature的概率

P(Green|B)- B类中有绿色feature的概率

P(A|Blue) = P(Blue|A)P(A) / P(Blue) = P(Blue|A)P(A) / (P(Blue|A)*P(A) +P(Blue|B)*P(B))

以宝可梦为例

Water and Normal type with ID < 400 for training. Rest for testing.

Training: 79 water, 61 Normal.

P(Water) = 79/(79+61) = 0.5643

P(Normal) = 61/(79+61) = 0.4357

Each Pokémon is represented as a vector by its attribute.

Vector = feature

Considering **Defense** and **SP Defense**

***下方为图片**

  

Each Pokémon is represented as a vector by its attribute.

此时，我们应该找水类，有杰尼龟feature的概率。

我们只考虑【防御力】和【SP防御力】这两个feature。那么如何local出现杰尼龟的概率呢？——这时候我们就该introduce正态分布模型（或者任何其他概率模型）了。

那么，如何确认Gaussian分布的参数呢？Maximum Likelihood思想。

The parameters of the Gaussian should maximize all the samples.

按照最大似然的思想，（理想）参数都能够用样本值计算出来。

Modifying Model

让不同feature的Gaussian的co-variance matrix变成一样的，这样子可以减少参数的数量。

而且，这样做是为了防止【过拟合】，大幅的提高了准确率。

Classification also applies to the "Three Steps"

- Function set: the function is organized in probability.
- Goodness of a function: the mean mu and covariance that maximizing the likelihood (the probability of generating data)
- Find the best function.

为什么要选Gaussian distribution

- You can always use the distribution you like.
- If you have a binary feature, you may assume they're from Bernoulli distributions.

***下方为图片

  

最后通过推导 z 的表达式，发现它能够被表示成：

Z = w*x + b

P(C1|x) = sigma(w*x + b);
What is logistic regression?

Logistic regression is a statistical model that is commonly used for predicting the probabilities of different outcomes in a binary classification problem. Unlike standard linear regression, which predicts continuous outcomes, logistic regression predicts categorical outcomes—specifically, binary outcomes (0 or 1, yes or no, true or false).

逻辑回归（Logistic Regression）是一种广泛使用的统计模型，主要用于处理分类问题，尤其是二分类问题。虽然它的名字中包含“回归”，但逻辑回归实际上是一个预测概率的分类方法。它通过使用一个逻辑函数（通常是Sigmoid函数）来估计概率，这使得它在预测分类结果（如是/否，成功/失败等）方面非常有效。

**Step 1: Function Set**

f_wb(x) = P_wb(C1|x)

**Step 2: Goodness of a Function**

Assume the data is generated based on f_wb(x)

Given a set of w and b, what is the probability of generating the data? The most likely w* and b* is the one with the largest L(w,b).

可以这么理解：对于逻辑回归问题，Loss function转变成了最大似然来求解最佳参数值。

如果我们把样本看作服从伯努利分布的样本的话，那么Loss function最终的形式实际是伯努利分布的cross entropy.

对比来看，linear regression里我们使用squared error来当作loss function的。

Bernoulli Cross Entropy, often referred to as Binary Cross Entropy in the context of binary classification, is a loss function used frequently in logistic regression when modeling binary outcome variables. This function measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label.

BERNOULLI CROSS ENTROPY EXPLAINED:

Bernoulli Cross Entropy loss, or Binary Cross Entropy, is calculated using the following formula:

L(y,p)=−[ylog(p)+(1−y)log(1−p)]

This formula evaluates the log loss for a single training example:

RELATION TO LOGISTIC REGRESSION:

In logistic regression, the goal is to model the probability that a given input belongs to the positive class (class labeled as 1). The output of logistic regression is a probability computed using the sigmoid function, which ensures the output is between 0 and 1. The model is trained by adjusting the weights to minimize the error in prediction, where error is quantified using the Bernoulli Cross Entropy loss.

The process involves:

 1. Computing the Probability:

   * Logistic Regression model predicts a probability ppp using the logistic function based on input features.

 2. Calculating Loss:

   * The Binary Cross Entropy loss is calculated for each training instance using the predicted probability ppp and the actual label yyy.

 3. Updating the Model:

   * The parameters (weights) of the model are updated in a way that minimizes the average cross-entropy loss across all training examples. This is typically done using optimization techniques such as Gradient Descent.

IMPORTANCE IN TRAINING:

Bernoulli Cross Entropy is a crucial component in training logistic regression models as it provides a measure of "how wrong" the model's predictions are. By minimizing this loss, the model is effectively trained to produce probabilities that accurately reflect the likelihood of an instance belonging to the positive class.

SUMMARY:

Bernoulli Cross Entropy is directly tied to the effectiveness of logistic regression in binary classification tasks. It allows the model to quantify the error in its predictions and provides a pathway for model improvement through optimization. This relationship is fundamental in machine learning for tasks like spam detection, medical diagnosis, and any other scenario where outcomes are binary.

Why no Square Error + Logistic Regression

*下面有图片

  

Cross Entropy vs Square Error

从函数形状来判断，Square Error就不适合做分类（配合梯度下降的算法），因为在远处它的曲率也非常平坦。

Discriminative vs Generative Models

The same model (function set), but different function is selected by the same training data.

判别式模型：直接follow sigma function，然后找对应的w and b （这个模型其实对数据分布没有任何的假设）

生成式模型：还是用gaussian的方式找合适的参数（我们其实有假设是按照高斯概率分布的）

Will we find the same set of w and b?

我们是做了不同的假设，所以根据同样的training data找出来的参数也会不同。

为什么在实践中发现，判别式模型的performance要比生成式的更好？Naïve Bayes

基于贝耶斯算法，会受到数据集中样本的**数量**影响；或者是脑补feature 1和feature 2之间有一定关联性

Generative model会做一件事情：脑补

Benefit of generative model

- With the assumption of probability distribution, less training data is needed.
- With the assumption of probability distribution, more robust to the noise.
- **Priors** and **class-dependent** probabilities can be estimated from different source. - 语音识别中的应用

Multi-class classification

C1: w1, b1; z1 = w1*x + b1

C2: w2, b2; z2 = w2*x + b2

C3: w3, b3; z3 = w3*x + b3

通过softmax方程的处理，得到三个class的后验概率（posterior possibility）y1y2y3。

这时我们再算target y e.g. [1,0,0], 以及他们之间的cross entropy.

Logistic regression's boundaries are actually the limitations…

Feature transformation

可以将数据转移到另一个feature space上面去

麻烦的事情是，有时你不知道该怎么做feature transformation

如何让机器自己generate这样的transformation？把很多logistic regression cascade起来

  

不同的regression进行组合以及cascade，到最后实际上也就是神经网络。